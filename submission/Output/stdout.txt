Using xla:1 device
### Reading and Encoding ###
Total lines in infile:  6739258
Dropped for N count:  23533
Outside length spec:  110138
Shape of X [N, C, H, W]: torch.Size([256, 4, 110, 1])
Shape of y: torch.Size([256]) torch.float32
Total training instances: 5945028
{'self': CNN(), 'feature_height': 110, 'feature_width': 1, 'batch_size': 256, 'print_size': True, 'out_channels': [512, 512, 512, 512, 512, 512], 'kernels': [(10, 1), (10, 1), (10, 1), (10, 1), (10, 1), (10, 1)], 'pool_kernels': [(1, 1), (1, 1), (1, 1), (1, 1), (10, 1), (1, 1)], 'paddings': 'same', 'strides': [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)], 'pool_strides': [(1, 1), (1, 1), (1, 1), (1, 1), (4, 1), (1, 1)], 'dropouts': [0.3, 0.3, 0.3, 0.3, 0.3, 0.3], 'linear_output': [256, 256], 'linear_dropouts': [0.0, 0.0], '__class__': <class '__main__.CNN'>}
Sizes: 110 x 1	110 x 1	110 x 1	110 x 1	26 x 1	26 x 1	Linear input size: 13312
CNN(
  (CNN_layers): ModuleList(
    (0): Sequential(
      (0): Conv2dSame(
        (conv): Conv2d(4, 512, kernel_size=(10, 1), stride=(1, 1))
      )
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (1): Sequential(
      (0): Conv2dSame(
        (conv): Conv2d(512, 512, kernel_size=(10, 1), stride=(1, 1))
      )
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (2): Sequential(
      (0): Conv2dSame(
        (conv): Conv2d(512, 512, kernel_size=(10, 1), stride=(1, 1))
      )
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (3): Sequential(
      (0): Conv2dSame(
        (conv): Conv2d(512, 512, kernel_size=(10, 1), stride=(1, 1))
      )
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (4): Sequential(
      (0): Conv2dSame(
        (conv): Conv2d(512, 512, kernel_size=(10, 1), stride=(1, 1))
      )
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool2d(kernel_size=(10, 1), stride=(4, 1), padding=0, dilation=1, ceil_mode=False)
      (4): Dropout(p=0.3, inplace=False)
    )
    (5): Sequential(
      (0): Conv2dSame(
        (conv): Conv2d(512, 512, kernel_size=(10, 1), stride=(1, 1))
      )
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
  )
  (linear_layers): Sequential(
    (0): Sequential(
      (0): Linear(in_features=13312, out_features=256, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.0, inplace=False)
    )
    (1): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.0, inplace=False)
    )
    (2): Sequential(
      (0): Linear(in_features=256, out_features=1, bias=True)
    )
  )
)
16611073
### Model Training ###
		T loss	V loss	T r	V r	T rho	V rho
Epoch 1:	1.3174	1.3255	0.6716	0.7136	0.6909	0.7293	*
Epoch 2:	1.2353	1.1921	0.7153	0.732	0.7315	0.7503	*
Epoch 3:	1.2092	1.1782	0.7267	0.7382	0.7428	0.7552	*
Epoch 4:	1.1962	1.1802	0.7318	0.7387	0.748	0.7571	*
Epoch 5:	1.1876	1.1669	0.735	0.7423	0.7513	0.7594	*
Epoch 6:	1.1806	1.1731	0.7376	0.7436	0.7539	0.7605	*
Epoch 7:	1.1754	1.1589	0.7396	0.7445	0.7559	0.7619	*
Epoch 8:	1.171	1.1576	0.7412	0.7454	0.7576	0.7627	*
Epoch 9:	1.1671	1.1574	0.7427	0.7452	0.7591	0.7624
Epoch 10:	1.1634	1.1589	0.7439	0.7455	0.7603	0.7633	*
Epoch 11:	1.1606	1.165	0.7447	0.7463	0.7613	0.7635	*
Epoch 12:	1.1577	1.1537	0.7457	0.7467	0.7623	0.7641	*
Epoch 13:	1.1551	1.1692	0.7467	0.7463	0.7633	0.7642
Epoch 14:	1.1528	1.1625	0.7475	0.7463	0.764	0.7638
Epoch 15:	1.1507	1.1561	0.7482	0.7469	0.7648	0.7648	*
Epoch 16:	1.1487	1.1576	0.7489	0.7465	0.7655	0.7642
Epoch 17:	1.1466	1.159	0.7495	0.7467	0.7661	0.7647
Epoch 18:	1.1451	1.1608	0.75	0.7475	0.7667	0.7648	*
Epoch 19:	1.1429	1.1558	0.7508	0.7463	0.7675	0.764
Epoch 20:	1.1411	1.1559	0.7513	0.7464	0.7681	0.7639
Epoch 21:	1.1399	1.1576	0.7517	0.7471	0.7684	0.7647
Epoch 22:	1.1381	1.1542	0.7524	0.7468	0.769	0.7644
Epoch 23:	1.1366	1.1688	0.7528	0.7457	0.7695	0.7637
Epoch 24:	1.12	1.1509	0.7581	0.7481	0.7748	0.7655	*
Epoch 25:	1.1161	1.15	0.7594	0.748	0.7761	0.7654
Epoch 26:	1.1138	1.1552	0.7601	0.7479	0.7769	0.7653
Epoch 27:	1.1123	1.1511	0.7606	0.7478	0.7774	0.7652
Epoch 28:	1.111	1.1509	0.761	0.7478	0.7778	0.7653
Epoch 29:	1.1096	1.151	0.7614	0.7479	0.7783	0.7653
Epoch 30:	1.1085	1.151	0.7618	0.7479	0.7786	0.7653
Epoch 31:	1.1074	1.15	0.7621	0.7481	0.779	0.7654
Epoch 32:	1.1064	1.1534	0.7624	0.7476	0.7793	0.7651
Epoch 33:	1.1058	1.1539	0.7626	0.747	0.7795	0.7647
Epoch 34:	1.105	1.1559	0.7628	0.7474	0.7797	0.7651
Early stop, best epoch 24
Predicting test set...
Total lines in infile:  71103
Dropped for N count:  0
Outside length spec:  0
{'self': CNN(), 'feature_height': 110, 'feature_width': 1, 'batch_size': 256, 'print_size': True, 'out_channels': [512, 512, 512, 512, 512, 512], 'kernels': [(10, 1), (10, 1), (10, 1), (10, 1), (10, 1), (10, 1)], 'pool_kernels': [(1, 1), (1, 1), (1, 1), (1, 1), (10, 1), (1, 1)], 'paddings': 'same', 'strides': [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)], 'pool_strides': [(1, 1), (1, 1), (1, 1), (1, 1), (4, 1), (1, 1)], 'dropouts': [0.3, 0.3, 0.3, 0.3, 0.3, 0.3], 'linear_output': [256, 256], 'linear_dropouts': [0.0, 0.0], '__class__': <class '__main__.CNN'>}
Sizes: 110 x 1	110 x 1	110 x 1	110 x 1	26 x 1	26 x 1	Linear input size: 13312
### Prediction Summary ###
                  0
count  71103.000000
mean      11.788829
std        2.605374
min        3.018863
25%        9.800974
50%       11.701784
75%       13.960765
max       19.958328
< 0:	 0
Submission file "submission20220801-130509.json" has been prepared.
Run time:  57786.908160209656
